<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Event Scheduling</title>
    <link rel="stylesheet" href="/styles/styles.css">
    <script src="/js/script.js" defer></script>
</head>

<body>
<div class="thin-wrapper">

    <h1>Motivation</h1>

    <p>
        Before getting a solution to this question, the fact that the due dates are abitrary real values seems to throw a wrench in the gears. We can think about this question clearly when the due dates are natural numbers because then we know that we can place our events exactly on integers, otherwise we could place them in any position, this greatly complicates things. To deal with this we will note the following:
    </p>

    <p>
        Suppose that we have any optimal solution \( O = \left( o _ 1, o _ 2, \ldots o _ m \right)  \), we claim that a new solution can be produced which has the same profit. This new solution is one where the solution only schedules at integer values, less than or equal to their original start time.
    </p>

    <p>
        We can prove this true with induction using the following claim "We can edit the original optimal solution up to index \( i \) by moving events leftward and snapping them to integer values while maintaining optimality". We'll sketch the proof, in the base case we know that we can move our first scheduled event all the way to \( 0 \), this is because \( 0 \) is an integer less than or equal to all start times, also moving this event backward will never collide with anything else because there are no events before it, it also maintains the same profit since \( o _ 1 ^ \prime \) is still scheduled before it's due date.
    </p>

    <p>
        In the induction step we assume that we can do this up to event \( k \), now we note that the finish time of event \( k \) is at some integer \( j \), it's not hard to see that for event \( k + 1 \) it can be slid back to start at time \( j \) or any integer between  \( j \)  and it's original start time, because it will have nothing to collide with there and it maintained it's deadline.
    </p>

    <p>
        Therefore we know that given any sequence of deadlines, we have some optimal solutions which all yield the same profit, we can then select one of them and "floor" it. Notationally, we'll denote this optimal solution by \( \lfloor O \rfloor  \). The reason why this is good is that we can try to come up with an algorithm that schedules events at integer values and it's still possible for this algorithm to be optimal.
    </p>

    <p>
        We will also note that for any optimal solution it will use at most \( n \) jobs, and thus any optimal solution can be contracted to a solution taking up at most \( n \) integers (by moving back deadlines).
    </p>

    <h1>Algorithm</h1>

    <p>
        Take the list of jobs and sort them by profit by highest profit first. Now we iterate through these jobs and schedule them at the latest possible time before their deadline where this time is an integer.
    </p>

    <h1>Pseudocode</h1>

    <pre>

        class ScheduleTracker:
            union_find = UnionFind()

            sets = []

            int earliest_scheduled_time = inf
            schedule = []

            def init(deadlines):
                latest_deadline = max(deadlines)
                for i in range(0, latest_deadline + 1):
                    sets.append(union_find.make_set(i))

            def set_event(time, event):
                union_find.union(sets[time - 1], sets[time])
                earliest_scheduled_time = event.start_time
                schedule.append(event)

            def get_latest_possible_schedule_time(event):
                return union_find.find(event.deadline)

            def is_possible_to_schedule(event):
                return get_latest_possible_schedule_time(event) != 0

        def maximize_profits(events):
            events = events_sorted_non_increasing_by_profit(events)

            schedule_tracker = ScheduleTracker()

            for event in events:
                if schedule_trakcer.is_possible_to_schedule(event):
                    time = schedule_tracker.get_latest_possible_time(event)
                    schedule_tracker.set_event(time, event)

            return schedule_tracker.get_profit()
    </pre>

    <h1>Correctness</h1>

    <p>
        We'll start by showing this yields an optimal solution. We'll end with time complexity. The proof of correctness of this algorithm is not trivial, we will employ some additional machinery to make our life easier.
    </p>

    <p>
        Let \( \mathcal{ J } := \left( J _ 1, J _ 2, \ldots, J _ n \right)  \) be the jobs sorted by non-increasing profits, let \( S _ i \) be the schedule that greedy has come up with after \( i \) iterations of the algorithm. Now observe this statement:
    </p>

    <p>
        \( \alpha _ i \)  : "\( S _ i \) can be extended to an optimal schedule by only adding jobs from the set \( \left\{ J _ k : i + 1 \le k \le n \right\}\)".
    </p>

    <p>
        When \( i = n \) in the above statement it reads, \( S _ n \) can be extended to an optimal schedule only adding jobs from \( \emptyset  \), which means that \( S _ n \) is optimal. Since after \( n \) iterations greedy will terminate, then \( S _ n \) is what is returned by greedy, so it will return an optimal solution.
    </p>

    <p>
        Therefore to prove that our algorithm is optimal, we can prove the above statement up to \( n \), we'll use induction for this.
    </p>

    <h2>Base Case</h2>

    <p>
        We want to show that \( \alpha _ 0 \) holds true, that is, we want to show that \( S _ 0 \) can be extended to an optimal schedule by only adding jobs from the set \( \left\{ J _ k : 1 \le k \le n \right\}  \). Suppose \( O \) is the optimal solution given these jobs, and consider the "floored" optimal solution \( \lfloor O \rfloor  \), After zero iterations of the greedy algorithm have occurred, nothing yet has happened, therefore nothing is scheduled \( S _ 0  = \emptyset \), then we can trivially just extend this by constructing \( S _ 0 \cup O  = O \) so it is an optimal extension
    </p>

    <h2>Induction Step</h2>

    <p>
        Now let \( j \in \mathbb{ N } _ 1 \), and assume that \( \alpha _ j \) holds true, we'd like to show that \( \alpha _ { j + 1 }  \) holds true. That is we must prove that \( S _ { j + 1 }  \) can be extended to an optimal schedule by only using jobs from \( \left\{ J _ k: j + 2 \le k \le n \right\}  \). Note that our induction hypothesis assumes that \( S _ j \) could be extended to an optimal solution only using jobs \( \left\{ J _ k : j + 1 \le k \le n \right\}  \), let's denote this optimal solution by \( O _ j \)
    </p>

    <p>
        At this point, we will now do cases based on how our algorithm operates. First of all when we consider the job \( J _ { j + 1 }  \), there are two possibilities, either it's not able to fit into the schedule, or it is. Suppose it cannot be scheduled, then this means \( S _ { j + 1 } = S _ j \) because our schedule has stayed the same, but we know that \( S _ j \) can be extended to \( O _ j \), since \( S _ j \) is the same as \( S _ { j + 1 }  \), then it can also be extended to \( O _ j \).
    </p>

    <p>
        Another option is that \( J _ { k + 1 }  \) is scheduled by greedy. If it is scheduled it is scheduled at some time \( t _ { j + 1 } \) (a natural number). We must also see that \( O _ j \) either contains \( J _ { k + 1 }   \) or it does not. If \( O _ j \) does contain \( J _ { k + 1 }  \), then we note that first of all, it is added in a way so that it doesn't collide with anything in \( S _ j \) because it was an extension, and since it is a solution we know that it scheduled it at a time before or equal to its deadline, let's denote this time as \( t _ O \)
    </p>

    <p>
        At the same time, greedy also did the same thing, it took \( J _ { j + 1 }  \) and it scheuled it in a way that extended \( S _ j \) and did not collide with anything previously, and it also scheduled it at a time before or equal to it's deadline, with the one extra assumption that it is scheduled at the <b>latest possible time</b>, this implies that \( t _ O \le  t _ { j + 1 }  \). Quickly see that if \( t _ O = t _ { j + 1 } \) then they've scheduled the job \( J _ { j + 1 }  \) at the exact same position,  and thus can be extended by taking \( S _ j \)'s extension and removing \( J _ { j + 1 }  \) (which is fine because it's already scheduled in \( S _ { j + 1 }  \), in otherwords we have an extension of \( S _ { j + 1 }  \) to an optimal solution using only \( \left\{ J _ k : i + 2 \le k \le n \right\}  \)
    </p>

    <p>
        Now for the final case, we have that \( S _ { j + 1 }  \) did schedule \( J _ { j + 1 }  \) but \( O _ j \) did not schedule it. Recall that greedy scheduled this job at time \( t _ { j + 1 }  \), we'll now show that \( O _ j \) must have something scheduled at that time, for if it did not, then since \( O _ j \) is a solution where each event is scheuled on integer values, we know that \( O _ j \) can fit in job \( J _ { j + 1 }  \) while not overlapping another event, this means that this new solution is still feasable, but it's profit has increased, but \( O _ j \) was optimal, this is a contradiction, so therefore \( O _ j \) must have something scheduled at time \( t _ { j + 1 }  \).
    </p>
    <p>
        Suppose \( O _ j \) has a job \( J _ m \) scheduled at time \( t _ { j + 1 }  \), we note that \( m \neq j + 1 \) because we're under the assumption that \( O _ j \) didn't schedule that job. But we know that \( J _ { j + 1 }  \) was not scheuled yet by \( S _ j \), this means it was part of the extension to \( O _ j \), therefore we have \( j + 1 \lt m \le n \), recall that our jobs \( \mathcal{ J }  \) were sorted in non-decreasing order which means that \( p \left( J _ { j + 1 }  \right) \ge p \left( J _ m \right) \), let's create a new solution where we take \( O _ j \) and replace \( J _ m \) with \( J _ { j + 1 }  \) denote this solution by \( O ^ \star \), note that \( p \left( O ^ \star \right) = p \left( O _ j \right) + p \left( J _ { j + 1 }  \right) - p \left( J _ m \right) \ge p \left( O _ j \right)  \), note that the only way to avoid contradiction is to have that \( p \left( J _ { j + 1 }  \right) = p \left( J _ m \right)   \), if that's true then \( O ^ \star \) has the same profit as \( O _ j \) therefore it is optimal. Note that \( O ^ \star \) included \( S _ { j + 1 }  \) and extends it to an optimal solution only using jobs \( \left\{ J _ k : j + 2 \le k \le n \right\}  \) which proves what we set out to show.
    </p>

    <h2>Schedule Tracker</h2>

    <p>
        We also have to prove that <code>ScheduleTracker</code> actually returns correct values as well. Since we've already have a formal proof for why the overall algorithm works, we'll sketch the proof here. Note that <code>is_possible_to_schedule</code> is correct if <code>get_latest_possible_schedule_time</code> is correct.
    </p>

    <p>
        For convience let \( L \left( t \right) \) denote: latest time to scehdule a job before or equal to time \( t \). We can see that it must satisfy the following
        \[
        L(t) =
        \begin{cases}
        t,  & \text{if no job scheduled at time $t$} \\
        L \left( t - 1 \right) , & \text{if there is a job scheduled at time $t$}
        \end{cases}
        \]
        Since our algorithm returns the find of a time, then we want to show that <code>find(t)</code> = \( L \left( t \right)  \) in any situation. That is that the representative of any set is \( L \left( t \right)  \) 
    </p>
    
    
    <p>
        We first iterate over all integer times before the last deadline and make a set in the union find with that specific time, this means that each time is it's own representative, we can see that now given some integer time \( t \) before any jobs are scheduled, then \( L \left( t \right) = t \)  which is exactly <code>find(t)</code> as needed.
    </p>

    <p>
        Assume that <code>find(t) = \( L \left( t \right)  \) </code>  on a schedule with \( k \) jobs scheduled, now we'll show it holds true on a schedule with \( k + 1  \) jobs scheduled. Suppose that our algorithm has just added one new job \( J ^ \star \) at time \( t ^  \star \) , by the algorithms we only schedule at position that was not already scheduled this means \( L \left( t ^ \star \right) = t ^ \star \), when there were \( k \) jobs our algorithm unions the set \( t ^ \star \) is in \( S ^ \star \) with the one at time \( t ^ \star - 1 \) called \( S ^ { \star - 1 }   \) this creates the new set \( S := S ^ { \star - 1 } \cup S ^ \star  \) the representative of \( S \) is the representative in \( S ^ { \star - 1 }  \), denote this representative by \( r \)
    </p>
    
    <p>
        Note that given any time in \( t \), it's representative only changed if it was an element in \( S ^ \star \). For any one of these times, \( t ^ \star \) used to be their representative, but now their representative is \( r \), we'll prove that \( r = L \left( t \right)  \), which shows the statement. Recall that before the union we used ot have \( L \left( t \right) = t ^ \star \), but now we have a job at time \( t ^ \star \), therefore now we have that \( L \left( t \right) = L \left( t ^ \star - 1 \right)  \), this can be seen because slot \( t ^ \star \) is filled so we can only choose the latest time coming before it. But \( t ^ \star - 1 \in S ^ { \star - 1 }  \) and so by the induction hypothesis \( L \left( t ^ \star - 1 \right) =   \)  find( \( t ^ \star - 1 ) \)) = r as needed.
    </p>

    <p>
        Therefore we've shown that at any stage, the union-find method actually returns the latest time to schedule a job before or equal to time \( t \).
    </p>


    <h2>Runtime</h2>

    <p>
        The cost of this alg:
    </p>

    <ul>
        <li>sort them by profit: \( \mathcal{ O } \left( n \log _ 2 n \right)  \)</li>
        <li>iterate through each event, figure find the latest point to schedule before or equal to the deadline: Initially this would be n^2, but with the improvements given by the union find (using as many optimzations as possible) we can bring this nearly down to \( \mathcal{ O } \left( n \right)   \) (Ackerman's function which is pretty much less than 4 for all realistic values)</li>
    </ul>

    <p>
        Therefore we can keep this algorithm at nearly \( \mathcal{ O } \left( n \log _ 2 n \right)  \) (because of the ackerman function) instead of \( \mathcal{ O } \left( n ^ 2 \right)  \)
    </p>




</div>
</body>
