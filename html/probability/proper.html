<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
    <title>Introduction</title>
	
    <link rel="stylesheet" href="/styles/styles.css">
    <script src="/js/script.js" defer></script>
</head>

<body>
    <div class="thin-wrapper">

		<div class="definition" id="definition-process">
			<div class="title">Process</div>
			<div class="content">
				Let \( X \) be a set and \( \Omega  \) be a sample space, then a process \( W: X \to \Omega  \) is an object that generates "outcomes" \( w \) from the sample space \( \Omega \). \( W : w _ 1, w _ 2, \ldots , w _ n, \ldots  \)
			</div>
		</div>

		<div class="definition" id="definition-random-variable">
			<div class="title">Random Variable</div>
			<div class="content">
				Given a process \( W \), and a function \( f : \Omega \to \mathbb{ R }  \) then \( g  \circ W : X \to \mathbb{ R }   \) is said to be a random variable.
			</div>
		</div>
		
		<div class="definition" id="definition-random-variable-and-binary-operation-syntax-sugar">
			<div class="title">Random Variable and Binary Operation Syntax Sugar</div>
			<div class="content">
				Random Variable and Binary Operation Syntax Sugar: Suppose that \( X : \Omega \to \mathbb{ R }  \) is a random variable and that \( \star \) is a binary operation, then for any \( a \in \mathbb{ R }   \) we define
				\[
				X \star a := \left\{ o \in \Omega :  X \left( o \right) \star a  \right\}
				\]
			</div>
		</div>

		<p>
			The main thing to note with the above definition is that when we write something of the form \( R = 3 \), then this is a subset of our sample space and therefore we can take the probability of it.
		</p>

		<p>
			If you're throwing a dice on a table, then the entire universe could be the system, and then it generates the outcome say \( 1 \) when you roll it.
		</p>

		<p>
			A system \( W \) should be thought as a finite list of rules that when followed results in the creation of something physically observable
		</p>

		<div class="definition" id="definition-probability-density-function">
			<div class="title">Probability Density Function</div>
			<div class="content">
				The probability density function (pdf) of a random variable \( X: \Omega \to \mathbb{ R }  \) is the function
				\[
					f _ X \left( t \right) = P \left( X = t \right)
				\]
			</div>
		</div>


		

		<div class="definition" id="definition-sample-mean">
			<div class="title">Sample Mean</div>
			<div class="content">
				The sample mean value sequence for \(X=g(W)\):  for each \(n \in \mathbb{N}\), the sample mean over the first \(n\) trials is simply the arithmetic average of the function values over those \(n\) trials :
				\[
				\begin{aligned}
				\widehat{E}_n X & \stackrel{\text { or }}{=} \widehat{E}_n g(W) \stackrel{\text { or }}{=} \widehat{E}_n g \\
				& \stackrel{\text { defn }}{=} \frac{g\left(w_1\right)+\cdots+g\left(w_n\right)}{n} \stackrel{\text { or }}{=} \frac{\boldsymbol{\Sigma}_{i=1}^n g\left(w_i\right)}{n} \\
				& \stackrel{\text { or }}{=} \frac{x_1+\cdots+x_n}{n} \stackrel{\text { or }}{=} \frac{\boldsymbol{\Sigma}_{i=1}^n x_i}{n} \stackrel{\text { or }}{=} \bar{x}_n .
				\end{aligned}
				\]
			</div>
		</div>
		
		<div class="definition" id="definition-random-system">
			<div class="title">Random System</div>
			<div class="content">
				A system \( W \) is said to be a <b>random system</b> when 
				<ul>
					<li>\( \left( \widehat{E} _ n g \left( W \right) \right)   \) converges</li>
					<li>It always converges to the same value given a different realization \( w _ 1, w _ 2 \) </li>
				</ul>
			</div>
		</div>

		<div class="definition" id="definition-empirical-relative-frequencies">
			<div class="title">Empirical Relative Frequencies</div>
			<div class="content">
				For any indicator function \(g=I_A\) with \(A \subset \Omega\) we will get the usual sequence of sample averages, but now to be referred to as empirical relative frequencies (these particular averages quite obviously giving the simple proportion of times that \(A\) occurs in the first \(n\) trials, as \(n=1,2, \ldots\) )
				\[
				\widehat{P}_n(W \in A) \stackrel{\text { defn }}{=} \widehat{E}_n I_A(W)=\frac{I_A\left(w_1\right)+\cdots+I_A\left(w_n\right)}{n}, \quad n \in \mathbb{N}
				\]
			</div>
		</div>

		<div class="definition" id="definition-long-run-frequency">
			<div class="title">Long Run Frequency</div>
			<div class="content">
				\[
				P ( W \in A) := E I _ A (W) = \lim _ { n \to \infty } P _ n (W \in A)
				\]
			</div>
		</div>
		
		<div class="definition" id="definition-discrete-uniform-distribution">
			<div class="title">Finite Uniform Distribution</div>
			<div class="content">
				For any specific \( N \in \mathbb{N} \) , the random variable \( X \) is said to have a (finite discrete) uniform distribution on the sample space \( \Omega=\{1, \ldots, N\} \) - denoted: \( X \sim \) unif \(\{1, \ldots, N\}\) iff
				\[
					P(X=k)=1 / N, \quad k=1, \ldots, N .
				\]
			</div>
		</div>

		<div class="definition" id="definition-continuous-uniform-distribution">
			<div class="title">Continuous Uniform Distribution</div>
			<div class="content">
				The random variable \(U\) is said to have a (continuous) uniform distribution on the unit interval \([0,1]\) denoted by \( U \sim \text { unif }[0,1]\) iff
				\[
				P(U \leq u)=u \quad \forall 0 \leq u \leq 1 .
				\]
			</div>
		</div>

		<div class="definition" id="definition-conditional-probability">
			<div class="title">Conditional Probability</div>
			<div class="content">
				Let \( A, B \subseteq \Omega  \), such that \( P \left( B \right) \neq 0 \) then the conditional probability of \( A \) given \( B \) is defined as
				\[
				    P \left( A \mid B \right) = \frac{P \left( A \cap B \right) }{ P \left( B \right) } 
				\]
			</div>
		</div>
		
		<div class="corollary" id="corollary-intersection-as-conditional-probability">
			<div class="title">Intersection as Conditional Probability</div>
			<div class="content">
				Suppose that \( P \left( A \right) , P \left( B \right) \neq 0 \) then we have 
				\[
				    P \left( A \cap B \right) = P \left( A \mid B \right) P \left( B \right)
				\]
			</div>
			<div class="proof">
				
			</div>
		</div>

		<div class="corollary" id="corollary-symmetric-conditional-equation">
			<div class="title">Symmetric Conditional Equation</div>
			<div class="content">
				\[
					P \left( A \mid B \right) P \left( B \right) = P \left( B \mid A \right) P \left( A \right)
				\]
			</div>
			<div class="proof">
			</div>
		</div>


		<div class="proposition" id="proposition-probability-of-set-difference">
			<div class="title">Probability of Set Difference</div>
			<div class="content">
				Suppose that \( A, B \subseteq \Omega  \) then
				\[
				    P \left( A \setminus B \right) = P \left( A \right) - P \left( A \cap B \right)
				\]
			</div>
			<div class="proof">

			</div>
		</div>


		<div class="lemma" id="lemma-probability-of-event-via-partition">
			<div class="title">Probability of an Event through a Partition</div>
			<div class="content">
				Suppose that \( \mathcal{ X }  = \left\{ X _ 1,  X _ 2, \ldots  \right\}  \) is a partition of \( \Omega  \) then for any event \( E \) we have
				\[
				P \left( E \right) = \sum _ { i \in \mathbb{ N } _ 1 } P \left(  E \cap X _ i  \right)
				\]
			</div>
			<div class="proof">
				<p>
					Observe that \( E = E \cap \Omega = E \cap \left( \sqcup _ { i \in \mathbb{ N } _ 1 } X _ i  \right) = \sqcup _ { i \in \mathbb{ N } _ 1 } \left( E \cap X _ i \right)    \), therefore
					\[
					    P \left( E  \right) = \sum _ { i \in \mathbb{ N } _ 1 } P \left( E \cap X _ i \right)
					\]
				</p>
			</div>
		</div>

		<p>
			It follows as an easy corollary that it holds for finite partitions by seting the rest of the partition to be the empty set.
		</p>

		<div class="theorem" id="theorem-law-of-total-probability">
			<div class="title">Law of Total Probability</div>
			<div class="content">
				Suppose that \( \mathcal{ X }  = \left\{ X _ 1,  X _ 2, \ldots  \right\}  \) is a partition of \( \Omega  \) then for any event \( E \) we have
				\[
					P \left( E \right) = \sum _ { i \in \mathbb{ N } _ 1 } P \left( E \mid X _ i \right) P \left( X _ i \right)
				\]
			</div>
			<div class="proof">
				
			</div>
		</div>

		<div class="definition" id="definition-independence-of-events">
			<div class="title">Independence of Events</div>
			<div class="content">
				Given two events \( A, B \subseteq \Omega  \) we say that \( A \) and \( B \)re independent iff
				\[
				P \left( A | B \right) = P \left( A \right)
				\]
			</div>
		</div>
		
		<div class="corollary" id="corollary-independence-is-symmetric">
			<div class="title">Independence is Symmetric</div>
			<div class="content">
				Suppose that \( A \) and \( B \) are independent, then \( B \) and \( A \) are independent
			</div>
			<div class="proof">
				<p>
					We know that \( P \left( A \mid B   \right) =  P \left( A \right) \), we'd like to prove that \( P \left( B \mid A \right) = P \left( B \right)   \),
					\[
					    \begin{align}
					        P \left( B \mid A \right) &= \frac{P \left( A \cap B \right) }{ P \left( A \right) } \\
							&= \frac{P \left( A \mid B \right) P \left( B \right)  }{ P \left( A \mid B \right) } \\
							&= P \left( B \right) 
					    \end{align}
					\]
					on the second line we used the <a class="knowledge-link" href="/probability/proper.html#corollary-intersection-as-conditional-probability">intersection as conditional formula</a>
				</p>
			</div>
		</div>
		

		
		<div class="definition" id="definition-statistically-independent">
			<div class="title">Independent Random Variables</div>
			<div class="content">
				Suppose that \( X, Y \) are two random variables, we say that \( X \) and \( Y \) are independent and write \( X \perp Y \) iff given any \( A, B \subseteq \mathbb{ R }  \) such that \( P \left( Y \in B \right) \neq 0  \)  we have that
				\[
				    P \left( X \in A | Y \in B \right) = P \left( X \in A \right)
				\]
			</div>
		</div>

		<div class="corollary" id="corollary-independence-as-a-product">
			<div class="title">Independence as a Product</div>
			<div class="content">
				\[
				    A \perp B \iff P \left( X \in A \cap X \in B \right) = P \left( X \in A \right) P \left( X \in B \right)
				\]
			</div>
			<div class="proof">
				We know that the following holds by this fact:
				\[
					P \left( A \cap B \right) = P \left( A \mid B\right)  P \left( B \right)
				\]
				therefore since we assumed \( A \perp B \) we know that \( P \left( A \mid B \right) = P \left( A \right)   \) and thus we have
				\[
				    P \left( A \cap B \right) =  P \left( A  \right) P \left( B \right)
				\]
				as needed.
			</div>
		</div>
		
		<div class="corollary" id="corollary-independence-is-reflexive">
			<div class="title">Independence is Reflexive</div>
			<div class="content">
				\[
				    A \perp B \iff B \perp A
				\]
			</div>
			<div class="proof">
				Follows from the previous corollary.
			</div>
		</div>

		<div class="exercise" id="exercise-independence-practice">
			<div class="title">Independence Practice</div>
			<div class="content">
				Suppose that \( A, B \) are two events such that \( P \left( A \setminus B  \right) = P \left( B \right) = \frac{2}{5}    \) and \( A \perp B \):
				<ul>
					<li>Determine \( P \left( A \right)  \) </li>
					<li>Obtain the probabilities \( P \left( A \cap B \right), P \left( A \cap B ^ \complement \right) , P \left( A ^ \complement \cap B ^ \complement \right)   \) </li>
					<li>
						Determine the conditional probability \( P \left( A \setminus B \mid A \cap B \right)  \)
					</li>
				</ul>
			</div>
			<div class="proof">
				<p>
					We start by determining \( P \left( A \right)  \), firstly <a class="knowledge-link" href="/probability/proper.html#proposition-probability-of-set-difference">we know that</a> 
					\[
					    \begin{align}
					        \frac{2}{5} &= P \left( B \right) = P \left( A \setminus B \right) \\
							&= P \left( A \right) - P \left( A \cap B \right) \\
							&= P \left( A \right) - P \left( A  \right) P \left( B \right)  \\
							&= P \left( A \right) \left( 1 - P \left( B \right)  \right) \\
							&= P \left( A \right) \left( 1 - \frac{2}{5}  \right) \\
							&= P \left( A \right) \left( \frac{3}{5}  \right) 
					    \end{align}
					\]
					therefore we deduce that \( P \left( A \right) = \frac{2}{3}  \) 	
				</p>

			</div>
		</div>

		<div class="definition" id="definition-collection-of-independent-events">
			<div class="title">Collection of Independent Events</div>
			<div class="content">
				Suppose we have some collection \( A _ i, i \in I  \) of events for some index set \( I \), then we say that this collection of events are independent if

			</div>
		</div>


		<div class="definition" id="definition-bernoulli-trial">
			<div class="title">Bernoulli Trial</div>
			<div class="content">
				The random varaible \( Z \) is said to be a bernoulli trial denoted by \( Z \sym \operatorname{ bern } \left( p \right)  \) for \( 0 \le p \le 1 \) iff
				\[
				    P \left( Z = 0 \right) = p \qquad P \left( Z = 1 \right) = 1 - p
				\]
			</div>
		</div>

		<div class="definition" id="definition-sequence-of-identically-and-independently-distributed-random-varaibles">
			<div class="title">Sequence of Identically and Independently Distributed Random Variables</div>
			<div class="content">
				Let \( \textbf{ X } = \left( X _ i : i \in I \right)  \) where \( I \) is some index set. We say that \( \textbf{ X } \) is identically and independently distributed and write IID with distribution \( D \)
				<ul>
					<li>\( X _ i \stackrel{d}{=} D  \) for every \( i \in I \)  </li>
					<li></li>
				</ul>
			</div>
		</div>

		
		<div class="definition" id="definition-binomial-distribution">
			<div class="title">Binomial Distribution</div>
			<div class="content">
				The random varaible \( X \) is said to have a binomial distribion on \( n \) trials, with probability of success per trial \( p \) iff 
				\[
				    X \stackrel{d}{=} Z _ 1 + \cdots + Z _ n
				\]
				where \( Z _ 1, \ldots , Z _ n \) are IID and \( Z _ i \sym \operatorname{ bern } \left( p \right)  \) 
			</div>
		</div>
		

		






		
		
		
		
		
		



    </div>
</body>
