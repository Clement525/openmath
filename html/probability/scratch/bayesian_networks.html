<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Bayesian Networks</title>

    <script src="/js/script.js"></script>
    <link rel="stylesheet" href="/styles/styles.css">

</head>
<body>

<div class="thin-wrapper">

    <div class="exercise" id="exercise-bayes-burritos">
    	<div class="title">Bayes Burritos</div>
    	<div class="content">
            Bayesâ€™ Burrito Bowls is a food truck that serves burrito bowls (obviously). You can choose:
            <ul>
                <li>a type of rice, which can be; white, brown</li>
                <li>a type of bean, which can be; black, pinto, lima, or kidney</li>
                <li>any of the following toppings; guacamole, corn, salsa, sour cream, onion, lettuce, or cheese</li>
            </ul>
            <p>
                We model each type of burrito as a triple (R, B, T), where R and B are the types of rice and bean, and T is any subset of the toppings. The type of burrito a customer picks depends on his/her preferences. We can model such preferences using a probability distribution over (R, B, T ).
            </p>
            <ul>
                <li>How many different types of burrito bowls are there?</li>
                <li>
                    Eshan builds a burrito bowl as follows: He first chooses the type of rice and bean independently of each other. Based on both these choices, he chooses the toppings (the toppings are not chosen independently of each other). Provide an appropriate factorization of \( P _ \mathrm{ Eshan } \left( R, B, T \right) \)  and the number of values that must be known to compute it for any R, B, T.
                </li>
                <li>
                    Zafeer builds a burrito as follows: He first chooses type of rice. Based on this choice, he chooses
                    a type of bean. He chooses the toppings independently of the other choices (toppings are not
                    chosen independently of each other). Provide an appropriate factorization of \( P _ \mathrm{ Zafeer } (R, B, T )  \)
                    and the number of values that must be known to compute it for any R, B, T
                </li>
            </ul>
    	</div>
    	<div class="proof">
            <p>
                We'll first count the number of different burritos, we already know that there are \( 2 \cdot 4 = 8 \) rice-bean combinations, therefore we just have to figure out how many different topping choices there are, clearly we want to count the number of different subsets of the collection of toppings, by putting each topping in a tuple, and then using a binary tuple of the same length to produce a subset, then there \( 2 ^ 7 \) topping choices, this leads there to be
                \[
                    2 \cdot 4 \cdot 2 ^ 7 = 2 ^ {10}
                \]
                different burritos.
            </p>
            <hr>
            <p>
                Eshan chooses rice and beans independently of each other, this means that \( P \left( B \mid R \right) = P \left( B \right)   \). Thus we can start by using the product rule and then obtaining
                \[
                    P \left( R, B, T \right) = P \left( R \right) P \left( B \mid R \right) P \left( T \mid R, B \right) = P \left( R \right) \cdot P \left( B \right) P \left( T \mid R, B \right)
                \]
                Therefore we need to know \( 2 + 4 + 2 ^ 7 \cdot 2 \cdot 4 \) values to compute it for any \( R, B, T \) 
            </p>
            <hr>
            <p>
                In Zafeer's case, we know that \( P \left( T \mid R, B  \right) = P \left( T \right)   \), therefore using the same initial factorization as above and simplifying respectively we obtain that
                \[
                    P \left( R, B, T \right) = P \left( R \right) P \left( B \mid R \right) P \left( T \right)
                \]
                and therefore we only have to know \( 2 + 2 \cdot 4 + 2 ^ 7 \) values, to compute it for any \( R, B, T \) which is an improvement on our original size.
            </p>
    	</div>
    </div>

    <div class="exercise" id="exercise-bayesian-network-independence">
    	<div class="title">Bayesian Network Independence</div>
    	<div class="content">
    		Consider the following bayesian network:
            <ul>
                <li>Which variables are independent of \( I \) </li>
                <li>Which variables are conditionally independent of \( A \) given \( K \) </li>
                <li>Which variables are conditionally independent of \( B \) given \( E \) </li>
            </ul>
    	</div>
    	<div class="proof">

            <p>
                Before we start recall when variables are conditionally indedpent using d-separation. Let's recall what an active path is and an inactive path, which are defined in terms of active and in-active triples.
            </p>

            <hr>
            <p>
               Let's first see what variables are independent of \( I \), to do this we verify which nodes have no active paths from them to \( I \) since \( J, G \) have direct dependence, then they cannot be independent of \( I \), \( K \) has the active path \( J - G - K \) so it is not independent and the same for \( H \). Looking at \( F \) we see that it is blocked by \( G \), similarly this is true for \( E \), moreover since there is only one active path from \( G \) to \( I \) as it is the bottleneck, then every path from the variables \( A, B, E, D, F \) is blocked by the inactive triple \( I - G - E \) therefore there is no active path from any of these variables to \( I \) and thus \( \left\{ A, B, E, D, F \right\}  \) are independent of \( I \).
            </p>
            <hr>
            <p>
                Suppose we are given \( K \) and we want to find which variables are conditionally independent of \( A \) given \( K \) observe that we obtain active paths from \(  B, E, F, G, H \) so they are all not indepedent with respect to \( A \), by the "common effect", we obtain that \( D, F, I \) are not independent, similarly we obtain that \( J \) is not independent, therefore none of the variables are conditionally independent of \( A \) given \( K \).
            </p
            <hr>
            <p>
                We repeat the process this time for the situation where we are given \( E \) and we want to find what variables are conditionally independent of \( B \). By "the common effect rule", we deduce that \( D \) is not independent, on the other hand, any path which has B-E-G as a subpath will be an in-active path, given any variable \( V \in \left\{ F, G, H, I, J, K \right\}  \) any path from \( B \) to \( V \) will always have B-E-G as a subpath, and therefore there are no active paths, so that only the variables \( F, G, H, I, J, K \) are independent of \( B \) given \( E \).
            </p>
    		
    	</div>
    </div>

    <div class="exercise" id="exercise-bayesian-constructor">
    	<div class="title">Bayesian Construction</div>
    	<div class="content">
    		Draw a Bayesian Network over the varaibles \( W, X, Y, Z \) such that
            <ul>
                <li>\( X \) and \( Y \) are independent given \( Z \)</li>
                <li>\( Z \) and \( W \) are independent given \( X \) and \( Y \)</li>
            </ul>
    	</div>
    	<div class="proof">

            Consider the network
            \[
                W \to X \to Z \to Y
            \]
            <p>
                We can see that \( W \) and \( Y \) re independent given \( Z \) as their path is blocked by the inactive triple \( X \to Z \to Y \). We also see that \( Z \) and \( W \) are independent given \( X \) and \( Y \) because they are blocked by \( X \) in a similar manner.
            </p>

    	</div>
    </div>

    <div class="exercise" id="exercise-bayesian-network-computation">
    	<div class="title">Bayesian Network Computation</div>
    	<div class="content">
            <ul>
                <li>Compute \( P \left( F = \mathrm{ yes } \mid N = n, M = m \right)  \) </li> for all \( m \in M, n \in N \)
            </ul>
    	</div>
    	<div class="proof">
            Recall that
            \[
            P \left( F = \mathrm{ yes } \mid N = n, M = m \right) = \frac{P \left( F = \mathrm{ yes } , N = n, M = m \right) }{P \left( M = m, N = n \right) }
            \]
            By the law of total probability we have that
            \[
            \frac{P \left( F = \mathrm{ yes } , N = n, M = m \right) }{P \left( M = m, N = n \right) } =  \frac{\sum _ { t \in T, s \in S } P \left( M = m, N = n, T = t, S = s, F = \mathrm{ yes }  \right)}{\sum _ { T, S, F } P \left( M = m, N = n, T = t, S = s, F = f \right)  }
            \]
            For easy of notation, let's denote \( X = x \) simply by \( x \) and \( \sum _{ x \in X  } P \left( X = x \right)   \) as \( \sum _ X P \left( X \right)  \)  we can re-write our value of interest as
            \[
                \frac{\sum _ { T, S } P \left( m, n, T, S, f \right) }{ \sum _ { T, S, F } P \left( m, n, T, S, F \right) } 
            \]
            <p>
                Observe that by ordering our variables like \( M, N, T, S, F \) anything to the left of a given variable is an ancestor node to it. Recall that throught the chain rule we obtain that 
                \[
                P \left( m, n, T, S, f \right) = P \left( m \right) P \left( m | n \right) P \left( T \mid m, n \right) P \left( S | T, m, n \right) P \left( F | S, T, m, n \right) 
                \]
            </p>
            <p>
                Now also recall that law of Bayesian Networks which says that given parents, a varaible is independent of it's non-decendentants, in other words we can simplify the above product to: 
                \[
                P \left( m \right) P \left( m | n \right) P \left( T \mid m, n \right) P \left( S | T, m, n \right) P \left( F | S, T, m, n \right) = P \left( m \right) P \left( n \right) P \left( T \mid m, n \right) P \left( S \mid n \right) P \left( F | T, S \right) 
                \]
            </p>
            <p>
                But we are doing a summation over, this observe that we can nest the summation inside with a particular order
                \[
                \sum _ { T, S }  P \left( m, n, T, S, F \right) = P \left( m \right) P \left( n \right) \left( \sum _ S P \left( S \mid n \right) \left( \sum _ T P \left( F \mid T, S \right) P \left( T \mid m, n \right)  \right)   \right)
                \]
            </p>
            <p>
                Let us define the inner most sum as a function \( g _ 1 \left( S, F \right)  \) so that
                \[
                    g _ 1 \left( S, F \right) = P \left( F \mid T = e, S \right) P \left( T = e \mid M = m, N = n \right) + P \left( F \mid T = l, S  \right) \cdot P \left( T = l \mid M = m, N = n \right)
                \]
                where we've iterated over the possible values of \( T \) so that our summation has become 
                \[
                 P \left( m \right) P \left( n \right) \left( \sum _ S P \left( S \mid n \right) g _ 1 \left( S, F \right)    \right)
                \]
            </p>
            <p>
                Let's take a closer look at this new summation and define a function \( g _ 2 \left( F \right)  \) as
                \[
                    P \left( S = f \mid n \right) g _ 1 \left( S = f, F \right) + P \left( S = s \mid n \right) g _ 1 \left( S = s, F \right)
                \]
                so that the original summation is really just 
                \[
                    P \left( m \right) P \left( n \right) g _ 2 \left( F \right) 
                \]
            </p>
            <p>
                The question wants us to find \( P \left( F = \mathrm{ yes } \mid N = n, M = m \right)  \) for each choice of \( n, m \) so let's do that. Suppose that \( M = \mathrm{ train }  \) and \( n = 0 \), then
                \[
                \begin{align}
                g _ 1 \left( S, F \right) &= P \left( F \mid T = e, S \right) P \left( T = e \mid M = \mathrm{ train }, N = 0 \right) + P \left( F \mid T = l, S  \right) \cdot P \left( T = l \mid M = \mathrm{ train }, N = 0 \right)\\
                &= P \left( F \mid T = e, S \right) 0.9 + P \left( F \mid T = l, S  \right) 0.1
                \end{align}
                \]
                We now have to compute \( g _ 2 \) we have:
                \[
                    \begin{align}
                        g _ 2 \left( F \right) &= P \left( S = f \mid N = 0 \right) g _ 1 \left( S = f, F \right) + P \left( S = s \mid N = 0 \right) g _ 1 \left( S = s, F \right)\\
                &= 0.9 \cdot g _ 1 \left( S = f, F \right) + 0.1 \cdot  g _ 1 \left( S = s, F \right)
                    \end{align}
                \]
                Therefore the numerator of \( \alpha \) is given by 
                \[
                    \begin{align}
                        g _ 2 \left( F = y \right)  + g _ 2 \left( F = n \right) &= \left(  0. 9 \cdot g _ 1 \left( S = f, F = y \right)  \right) + \left( 0. 1 \cdot g _ 1 \left( S = s, F = n \right)  \right) \\
                        &= ...
                    \end{align}
                \]
                At this point we realize that this problem will require many computations, so as we are programmers, we write a program which encodes everything we've done up until this point and then use it to determine \( P \left( F = y \mid M, N \right)  \) for all \( M, N \). After observing the output of this program we deduce that you should take the train with no bags to maximize your changes of making the flight.
            </p>
    	</div>
    </div>


</div>


</body>
</html>