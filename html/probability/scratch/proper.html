<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
    <title>Introduction</title>
	
    <link rel="stylesheet" href="/styles/styles.css">
    <script src="/js/script.js" defer></script>
</head>

<body>
    <div class="thin-wrapper">

		<div class="definition" id="definition-process">
			<div class="title">Process</div>
			<div class="content">
				Let \( X \) be a set and \( \Omega  \) be a sample space, then a process \( W: X \to \Omega  \) is an object that generates "outcomes" \( w \) from the sample space \( \Omega \). \( W : w _ 1, w _ 2, \ldots , w _ n, \ldots  \)
			</div>
		</div>

		<div class="definition" id="definition-random-variable">
			<div class="title">Random Variable</div>
			<div class="content">
				Given a process \( W \), and a function \( f : \Omega \to \mathbb{ R }  \) then \( g  \circ W : X \to \mathbb{ R }   \) is said to be a random variable.
			</div>
		</div>
		

		<p>
			If you're throwing a dice on a table, then the entire universe could be the system, and then it generates the outcome say \( 1 \) when you roll it.
		</p>

		<p>
			A system \( W \) should be thought as a finite list of rules that when followed results in the creation of something physically observable
		</p>

		<div class="definition" id="definition-probability-density-function">
			<div class="title">Probability Density Function</div>
			<div class="content">
				The probability density function (pdf) of a random variable \( X: \Omega \to \mathbb{ R }  \) is the function
				\[
					f _ X \left( t \right) = P \left( X = t \right)
				\]
			</div>
		</div>


		

		<div class="definition" id="definition-sample-mean">
			<div class="title">Sample Mean</div>
			<div class="content">
				The sample mean value sequence for \(X=g(W)\):  for each \(n \in \mathbb{N}\), the sample mean over the first \(n\) trials is simply the arithmetic average of the function values over those \(n\) trials :
				\[
				\begin{aligned}
				\widehat{E}_n X & \stackrel{\text { or }}{=} \widehat{E}_n g(W) \stackrel{\text { or }}{=} \widehat{E}_n g \\
				& \stackrel{\text { defn }}{=} \frac{g\left(w_1\right)+\cdots+g\left(w_n\right)}{n} \stackrel{\text { or }}{=} \frac{\boldsymbol{\Sigma}_{i=1}^n g\left(w_i\right)}{n} \\
				& \stackrel{\text { or }}{=} \frac{x_1+\cdots+x_n}{n} \stackrel{\text { or }}{=} \frac{\boldsymbol{\Sigma}_{i=1}^n x_i}{n} \stackrel{\text { or }}{=} \bar{x}_n .
				\end{aligned}
				\]
			</div>
		</div>
		
		<div class="definition" id="definition-random-system">
			<div class="title">Random System</div>
			<div class="content">
				A system \( W \) is said to be a <b>random system</b> when 
				<ul>
					<li>\( \left( \widehat{E} _ n g \left( W \right) \right)   \) converges</li>
					<li>It always converges to the same value given a different realization \( w _ 1, w _ 2 \) </li>
				</ul>
			</div>
		</div>

		<div class="definition" id="definition-empirical-relative-frequencies">
			<div class="title">Empirical Relative Frequencies</div>
			<div class="content">
				For any indicator function \(g=I_A\) with \(A \subset \Omega\) we will get the usual sequence of sample averages, but now to be referred to as empirical relative frequencies (these particular averages quite obviously giving the simple proportion of times that \(A\) occurs in the first \(n\) trials, as \(n=1,2, \ldots\) )
				\[
				\widehat{P}_n(W \in A) \stackrel{\text { defn }}{=} \widehat{E}_n I_A(W)=\frac{I_A\left(w_1\right)+\cdots+I_A\left(w_n\right)}{n}, \quad n \in \mathbb{N}
				\]
			</div>
		</div>

		<div class="definition" id="definition-long-run-frequency">
			<div class="title">Long Run Frequency</div>
			<div class="content">
				\[
				P ( W \in A) := E I _ A (W) = \lim _ { n \to \infty } P _ n (W \in A)
				\]
			</div>
		</div>
		




		<div class="definition" id="definition-bernoulli-trial">
			<div class="title">Bernoulli Trial</div>
			<div class="content">
				The random varaible \( Z \) is said to be a bernoulli trial denoted by \( Z \sym \operatorname{ bern } \left( p \right)  \) for \( 0 \le p \le 1 \) iff
				\[
				    P \left( Z = 0 \right) = p \qquad P \left( Z = 1 \right) = 1 - p
				\]
			</div>
		</div>

		<div class="definition" id="definition-sequence-of-identically-and-independently-distributed-random-varaibles">
			<div class="title">Sequence of Identically and Independently Distributed Random Variables</div>
			<div class="content">
				Let \( \textbf{ X } = \left( X _ i : i \in I \right)  \) where \( I \) is some index set. We say that \( \textbf{ X } \) is identically and independently distributed and write IID with distribution \( D \)
				<ul>
					<li>\( X _ i \stackrel{d}{=} D  \) for every \( i \in I \)  </li>
					<li></li>
				</ul>
			</div>
		</div>

		
		<div class="definition" id="definition-binomial-distribution">
			<div class="title">Binomial Distribution</div>
			<div class="content">
				The random varaible \( X \) is said to have a binomial distribion on \( n \) trials, with probability of success per trial \( p \) iff 
				\[
				    X \stackrel{d}{=} Z _ 1 + \cdots + Z _ n
				\]
				where \( Z _ 1, \ldots , Z _ n \) are IID and \( Z _ i \sym \operatorname{ bern } \left( p \right)  \) 
			</div>
		</div>
		

		






		
		
		
		
		
		



    </div>
</body>
